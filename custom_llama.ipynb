{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compressed_llama.modeling_llama_c import LlamaModel, LlamaForCausalLM\n",
    "from compressed_llama.configuration_llama_c import LlamaConfig\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab68acbf54f04a80971a15e499120add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at meta-llama/Llama-2-7b-chat-hf were not used when initializing LlamaForCausalLM: ['model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['model.layers.14.down_proj.bias', 'model.layers.14.down_proj.weight', 'model.layers.14.down_proj_2.bias', 'model.layers.14.down_proj_2.weight', 'model.layers.14.input_layer.bias', 'model.layers.14.input_layer.weight', 'model.layers.14.norm.weight', 'model.layers.14.up_proj.bias', 'model.layers.14.up_proj.weight', 'model.layers.19.gate_proj.bias', 'model.layers.19.gate_proj.weight', 'model.layers.19.input_layer.bias', 'model.layers.19.input_layer.weight', 'model.layers.19.norm.weight', 'model.layers.19.up_proj.bias', 'model.layers.19.up_proj.weight', 'model.layers.19.up_proj_2.bias', 'model.layers.19.up_proj_2.weight', 'model.layers.32.input_layernorm.weight', 'model.layers.32.mlp.down_proj.weight', 'model.layers.32.mlp.gate_proj.weight', 'model.layers.32.mlp.up_proj.weight', 'model.layers.32.post_attention_layernorm.weight', 'model.layers.32.self_attn.k_proj.weight', 'model.layers.32.self_attn.o_proj.weight', 'model.layers.32.self_attn.q_proj.weight', 'model.layers.32.self_attn.v_proj.weight', 'model.layers.33.input_layernorm.weight', 'model.layers.33.mlp.down_proj.weight', 'model.layers.33.mlp.gate_proj.weight', 'model.layers.33.mlp.up_proj.weight', 'model.layers.33.post_attention_layernorm.weight', 'model.layers.33.self_attn.k_proj.weight', 'model.layers.33.self_attn.o_proj.weight', 'model.layers.33.self_attn.q_proj.weight', 'model.layers.33.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized because the shapes did not match:\n",
      "- model.layers.15.input_layernorm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- model.layers.15.mlp.down_proj.weight: found shape torch.Size([4096, 11008]) in the checkpoint and torch.Size([2048, 11008]) in the model instantiated\n",
      "- model.layers.15.mlp.gate_proj.weight: found shape torch.Size([11008, 4096]) in the checkpoint and torch.Size([11008, 2048]) in the model instantiated\n",
      "- model.layers.15.mlp.up_proj.weight: found shape torch.Size([11008, 4096]) in the checkpoint and torch.Size([11008, 2048]) in the model instantiated\n",
      "- model.layers.15.post_attention_layernorm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- model.layers.15.self_attn.k_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.15.self_attn.o_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.15.self_attn.q_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.15.self_attn.v_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.16.input_layernorm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- model.layers.16.mlp.down_proj.weight: found shape torch.Size([4096, 11008]) in the checkpoint and torch.Size([2048, 11008]) in the model instantiated\n",
      "- model.layers.16.mlp.gate_proj.weight: found shape torch.Size([11008, 4096]) in the checkpoint and torch.Size([11008, 2048]) in the model instantiated\n",
      "- model.layers.16.mlp.up_proj.weight: found shape torch.Size([11008, 4096]) in the checkpoint and torch.Size([11008, 2048]) in the model instantiated\n",
      "- model.layers.16.post_attention_layernorm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- model.layers.16.self_attn.k_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.16.self_attn.o_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.16.self_attn.q_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.16.self_attn.v_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.17.input_layernorm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- model.layers.17.mlp.down_proj.weight: found shape torch.Size([4096, 11008]) in the checkpoint and torch.Size([2048, 11008]) in the model instantiated\n",
      "- model.layers.17.mlp.gate_proj.weight: found shape torch.Size([11008, 4096]) in the checkpoint and torch.Size([11008, 2048]) in the model instantiated\n",
      "- model.layers.17.mlp.up_proj.weight: found shape torch.Size([11008, 4096]) in the checkpoint and torch.Size([11008, 2048]) in the model instantiated\n",
      "- model.layers.17.post_attention_layernorm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- model.layers.17.self_attn.k_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.17.self_attn.o_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.17.self_attn.q_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.17.self_attn.v_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.18.input_layernorm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- model.layers.18.mlp.down_proj.weight: found shape torch.Size([4096, 11008]) in the checkpoint and torch.Size([2048, 11008]) in the model instantiated\n",
      "- model.layers.18.mlp.gate_proj.weight: found shape torch.Size([11008, 4096]) in the checkpoint and torch.Size([11008, 2048]) in the model instantiated\n",
      "- model.layers.18.mlp.up_proj.weight: found shape torch.Size([11008, 4096]) in the checkpoint and torch.Size([11008, 2048]) in the model instantiated\n",
      "- model.layers.18.post_attention_layernorm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- model.layers.18.self_attn.k_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.18.self_attn.o_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.18.self_attn.q_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.18.self_attn.v_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = LlamaConfig()\n",
    "# model = LlamaModel(config)\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-13): 14 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14): CustomLlamaEncoderLayer(\n",
       "        (input_layer): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (up_proj): Linear(in_features=4096, out_features=8192, bias=True)\n",
       "        (down_proj): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "        (down_proj_2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "        (norm): LlamaRMSNorm()\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (15-18): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (19): CustomLlamaDecoderLayer(\n",
       "        (input_layer): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (gate_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (up_proj): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "        (up_proj_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (act_fn): ReLU()\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (20-33): 14 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaDecoderLayer 0 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 8, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 8, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 9, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 9, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 8, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 8, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 9, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 9, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 8, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 8, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 9, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 9, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 8, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 8, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 9, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 9, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 8, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 8, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 9, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 9, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 8, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 8, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 9, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 9, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 8, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 8, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 9, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 9, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 8, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 8, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 8, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 8, 4096])\n",
      "LlamaDecoderLayer 0 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 1 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 2 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 3 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 4 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 5 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 6 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 7 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 8 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 9 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 10 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 11 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 12 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 13 torch.Size([1, 9, 4096])\n",
      "CustomLlamaEncoderLayer 14 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 15 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 16 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 17 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 18 torch.Size([1, 9, 2048])\n",
      "CustomLlamaDecoderLayer 19 torch.Size([1, 9, 2048])\n",
      "LlamaDecoderLayer 20 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 21 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 22 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 23 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 24 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 25 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 26 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 27 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 28 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 29 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 30 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 31 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 32 torch.Size([1, 9, 4096])\n",
      "LlamaDecoderLayer 33 torch.Size([1, 9, 4096])\n",
      "7.43 s ± 304 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, my dog is cute\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout = model.generate(**tokens, max_length=10, use_cache=False)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mout\u001b[49m[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "tokens = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**tokens, max_length=10, use_cache=False)\n",
    "\n",
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from safetensors import safe_open\n",
    "\n",
    "# model_dir = \"../../root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/92011f62d7604e261f748ec0cfe6329f31193e33/model-00001-of-00002.safetensors\"\n",
    "\n",
    "# tensors = {}\n",
    "# with safe_open(model_dir, framework=\"pt\", device=\"cpu\") as f:\n",
    "#     for key in f.keys():\n",
    "#         tensors[key] = f.get_tensor(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
